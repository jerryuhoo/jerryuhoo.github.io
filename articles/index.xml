<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Articles on Wings Music</title><link>https://jerryuhoo.github.io/articles/</link><description>Recent content in Articles on Wings Music</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 13 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://jerryuhoo.github.io/articles/index.xml" rel="self" type="application/rss+xml"/><item><title>VISinger2+ End-to-End Singing Voice Synthesis Augmented by Self-Supervised Learning Representation</title><link>https://jerryuhoo.github.io/articles/3/</link><pubDate>Thu, 13 Jun 2024 00:00:00 +0000</pubDate><guid>https://jerryuhoo.github.io/articles/3/</guid><description>&lt;p>&lt;strong>Yifeng Yu&lt;/strong>, Jiatong Shi, Yuning Wu, Shinji Watanabe&lt;/p>
&lt;p>Singing Voice Synthesis (SVS) has witnessed significant advancements with the advent of deep learning techniques. However, a significant challenge in SVS is the scarcity of labeled singing voice data, which limits the effectiveness of supervised learning methods. In response to this challenge, this paper introduces a novel approach to enhance the quality of SVS by leveraging unlabeled data from pre-trained self-supervised learning models. Building upon the existing VISinger2 framework, this study integrates additional spectral feature information into the system to enhance its performance. The integration aims to harness the rich acoustic features from the pre-trained models, thereby enriching the synthesis and yielding a more natural and expressive singing voice. Experimental results in various corpora demonstrate the efficacy of this approach in improving the overall quality of synthesized singing voices in both objective and subjective metrics.&lt;/p></description></item><item><title>Singing Voice Data Scaling-up An Introduction to ACE-Opencpop and KiSing-v2</title><link>https://jerryuhoo.github.io/articles/1/</link><pubDate>Wed, 31 Jan 2024 00:00:00 +0000</pubDate><guid>https://jerryuhoo.github.io/articles/1/</guid><description>&lt;p>Jiatong Shi, Yueqian Lin, Xinyi Bai, Keyi Zhang, Yuning Wu, Yuxun Tang, &lt;strong>Yifeng Yu&lt;/strong>, Qin Jin, Shinji Watanabe&lt;/p>
&lt;p>In singing voice synthesis (SVS), generating singing voices from musical scores faces challenges due to limited data availability, a constraint less common in text-to-speech (TTS). This study proposes a new approach to address this data scarcity. We utilize an existing singing voice synthesizer for data augmentation and apply precise manual tuning to reduce unnatural voice synthesis. Our development of two extensive singing voice corpora, ACE-Opencpop and KiSing-v2, facilitates large-scale, multi-singer voice synthesis. Utilizing pre-trained models derived from these corpora, we achieve notable improvements in voice quality, evident in both in-domain and out-of-domain scenarios. The corpora, pre-trained models, and their related training recipes are publicly available at Muskits-ESPnet.&lt;/p></description></item><item><title>A Systematic Exploration of Joint-training for Singing Voice Synthesis</title><link>https://jerryuhoo.github.io/articles/2/</link><pubDate>Sat, 05 Aug 2023 00:00:00 +0000</pubDate><guid>https://jerryuhoo.github.io/articles/2/</guid><description>&lt;p>Yuning Wu, &lt;strong>Yifeng Yu&lt;/strong>, Jiatong Shi, Tao Qian, Qin Jin&lt;/p>
&lt;p>There has been a growing interest in using end-to-end acoustic models for singing voice synthesis (SVS). Typically, these models require an additional vocoder to transform the generated acoustic features into the final waveform. However, since the acoustic model and the vocoder are not jointly optimized, a gap can exist between the two models, leading to suboptimal performance. Although a similar problem has been addressed in the TTS systems by joint-training or by replacing acoustic features with a latent representation, adopting corresponding approaches to SVS is not an easy task. How to improve the joint-training of SVS systems has not been well explored. In this paper, we conduct a systematic investigation of how to better perform a joint-training of an acoustic model and a vocoder for SVS. We carry out extensive experiments and demonstrate that our joint-training strategy outperforms baselines, achieving more stable performance across different datasets while also increasing the interpretability of the entire framework.&lt;/p></description></item></channel></rss>